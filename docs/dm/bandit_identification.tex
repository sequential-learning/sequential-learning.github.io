% !TeX root = ./mva_sequential_learning_homework.tex

\section*{Part 2. Stochastic Best Arm Identification}
\label{sec:best_arm_identification}

In the best arm identification setting, an algorithm interacts with the environment in the standard bandit way: at each time, it selects an arm, then observes the corresponding reward. The goal of the algorithm is to find the arm with highest mean, as quickly as possible.


\begin{setting}[ht!]
\begin{center}
\fbox{
    \begin{minipage}{.95\textwidth}
    	Each arm $k \in [K]$ has a reward distribution $\mathcal \nu_k$ with mean $\mu_k$. There is a unique best arm $k^* = \arg\max_k \mu_k$.\\[2pt]
        At each round $t=1,\dots,\tau$
        \begin{itemize}
            \item The player chooses an arm $k_t \in [K]$,
            \item The player observes a reward $X_t^{k_t} \sim \mathcal \nu_{k_t}$, independent of all other rewards.
        \end{itemize}
        At the stopping time $\tau$, the algorithm recommends an arm $\hat{k} \in [K]$
    \end{minipage}}
\end{center}
\caption{Best arm identification}
\label{fig:setting_stochastic}
\end{setting}

A good algorithm should make few mistakes and stop quickly. With the notations of Figure~\ref{fig:setting_stochastic}, the probability of mistake of the algorithm on problem $\nu$ is $\mathbb{P}_\nu(\hat{k} \ne k^*)$. The possibly random time at which it stops and returns an answer is $\tau$.

Notations: $N_t^k = \sum_{s=1}^t \mathbb{I}\{k_s = k\}$ is the number of times arm $k$ was sampled up to time $t$. $\hat{\mu}_{t,k} = \frac{1}{N_t^k} \sum_{s=1}^t \mathbb{I}\{k_s = k\} X_s^{k_s}$ is the empirical mean of arm $k$.

\paragraph{Fixed Budget}
\label{par:fixed_budget}

In \emph{fixed budget} best arm identification, we are given a time $T$ and set $\tau = T$. That is, the algorithm can sample a total number of $T$ arms ($T$ known in advance), then must stop and return an answer. We are interested in algorithms with low probability of mistake.

\begin{questions}


\question \emph{Uniform sampling.} The uniform sampling algorithm pulls all arms $T/K$ times (we suppose that $T$ is a multiple of $K$).
%In this questions, we take $K = 10$, $T = 1000$ and all arm distributions are Gaussian with variance 1 and means $(1, 0.8, 0.6, 0.6, \ldots, 0.6)$. 
\begin{parts}
	\part Prove that the probability of error of uniform sampling is at most $2 \sum_{k=2}^K \exp(-\frac{T}{K} \frac{\Delta_k^2}{8})$, where $\Delta_k = \mu_{k^*} - \mu_k$.
\end{parts}

\begin{algorithm}[!h]
    \SetAlgoLined
    Input: budget $T$, number of arms $K$.\\
    Let $A_1 = \{1, \ldots, K\}$, $\overline{\log}(K) = \frac{1}{2} + \sum_{k=2}^K \frac{1}{k}$, $n_0 = 0$ and for $j \in \{1, \ldots, K-1\}$,
    \begin{align*}
    n_j = \left\lceil \frac{1}{\overline{\log}(K)}\frac{T-K}{K+1-j}\right\rceil \: .
    \end{align*}
    \For{each phase $j = 1, 2, \ldots, K - 1$}{
        For each $i \in A_j$, select arm $i$ during $n_j - n_{j - 1}$ rounds. \\
        Let $A_{j+1} = A_j \setminus \arg\min_{k \in A_j} \hat{X}_{k, n_j}$, where $\hat{X}_{k, n_j}$ is the empirical mean of arm $k$ after $n_j$ observations. (we only remove one element from $A_j$; if there is a tie, select randomly the arm to dismiss among the worst arms).
    }
    \caption{Successive rejects}
    \label{alg:successive_rejects}
\end{algorithm}

\question The \emph{Successive Rejects} algorithm is described in Algorithm~\ref{alg:successive_rejects}. In each phase, it samples all arms uniformly, and at the end of a phase it discards the worse arm.
\begin{parts}
    \part Give a bound on the probability that the best arm is discarded at the end of the first phase.
    \part Let $B = \mathbb{I}\{\hat{k} \ne k^*\}$ be the Bernoulli random variable with value 1 if the algorithm makes a mistake and 0 otherwise. Its expectation on the bandit problem $\nu$ is $\mathbb{P}_\nu(\hat{k} \ne k^*)$. Suppose that we run $n$ parallel experiments, and that in each experiment $i \in [n]$ we run successive rejects on the same bandit $\nu$ and compute the corresponding $B_i = \mathbb{I}\{\hat{k} \ne k^*\}$. Give a confidence interval for $\mathbb{P}_\nu(\hat{k} \ne k^*)$.
    \part Implement successive rejects and uniform sampling. Plot the probability of error of both algorithms for $K = 20$ Bernoulli arms with $\mu_1 = 0.5$ and $\mu_k = 0.4$ for $k \ge 2$, for $T \in \{100, 500, 2000\}$. Plot confidence intervals and make sure to use enough experiments to get intervals with smaller width than the error probability.
\end{parts}

\end{questions}

\paragraph{Fixed Confidence}
\label{par:fixed_confidence}

In \emph{fixed confidence} best arm identification, we consider only \emph{$\delta$-correct} algorithms, which satisfy $\mathbb{P}_\nu(\hat{k} \ne k^*) \le \delta$ for all tuples of distributions in our model. We are then looking for such algorithms with minimal expected stopping time $\mathbb{E}_\nu[\tau]$.

All arm distributions in this section will be Gaussian with variance 1. All experiments will use 10 such arms, with means $(0.5, 0.4, 0.4, 0.3, \ldots, 0.3)$. All experiments will use $\delta = 0.01$.

\emph{Stopping rule}. Let $\hat{*}_t = \arg\max_{k \in [K]} \hat{\mu}_{t,k}$. All algorithms will use the same stopping rule: stop when
\begin{align*}
\inf_{k \in [K] \setminus \{\hat{*}_t\}} \frac{1}{2} \frac{(\hat{\mu}_{t,\hat{*}_t} - \hat{\mu}_{t,k})^2}{\frac{1}{N_t^{\hat{*}_t}}+\frac{1}{N_t^k}}
> \log \frac{1}{\delta} + 3 \log (1 + \log t) \: .
\end{align*}
$\tau$ is the first time such that this condition is satisfied.
This is a heuristic approximation of the GLRT stopping rule seen in the course (the quantity on the left is equal to $\inf_{\lambda : *_\lambda \ne \hat{*}_t} \sum_{k=1}^K N_t^k \frac{1}{2}(\hat{\mu}_{t,k} -\lambda_{k})^2$).

\begin{questions}
\question In order to get a sub-linear regret, a regret minimization algorithms has to sample mostly the best arm. It could thus be transformed into a best arm identification algorithm.
\begin{parts}
    \part Implement the UCB algorithm for regret minimization, which samples the arm\\$k_t = \arg\max \hat{\mu}_{t-1,k} + \sqrt{\frac{2 \log t}{N_{t-1}^k}}$. Plot the mean over 100 experiments of the regret of UCB on the Gaussian bandit problem described above, for $t$ from 1 to 10000.
    \part Implement a best arm identification algorithm that samples like UCB, stops according to the rule presented above, and returns the most played arm. Implement another algorithm that samples all arms uniformly, stops according to the rule presented above, and returns the arm with highest empirical mean. On a box plot, compare the stopping times of both algorithms. 
\end{parts}

\question Regret minimization algorithms don't explore enough to be good identification methods. We can modify them to explore more: this is the idea of \emph{Top-Two} algorithms. A Top-Two algorithm computes at each time $t$ a \emph{leader} $B_t \in [K]$ and a \emph{challenger} $C_t \in [K]\setminus\{B_t\}$, and then with probability $\beta \in (0,1)$ it samples the leader ($k_t = B_t$), and it samples the challenger with probability $1 - \beta$. It stops according to the rule presented above, and recommends the arm with highest empirical mean. We will use $\beta = 1/2$.
\begin{parts}
    \part Implement the TTUCB algorithm: its leader is the UCB arm $B_t = \arg\max \hat{\mu}_{t-1,k} + \sqrt{\frac{2 \log t}{N_{t-1}^k}}$ and its challenger is the arm for which it is hardest to say that it is worse, $C_t = \arg\min_{k \ne B_t} \frac{1}{2} \frac{(\hat{\mu}_{t,B_t} - \hat{\mu}_{t,k})^2}{\frac{1}{N_t^{B_t}}+\frac{1}{N_t^k}}$.
    \part We added an exploration mechanism, the challenger, and it could be that using UCB for the leader is not necessary anymore. Implement the so-called EB-TC algorithm which uses $B_t = \arg\max \hat{\mu}_{t-1,k}$ and $C_t = \arg\min_{k \ne B_t} \frac{1}{2} \frac{(\hat{\mu}_{t,B_t} - \hat{\mu}_{t,k})^2}{\frac{1}{N_t^{B_t}}+\frac{1}{N_t^k}}$.
    \part On a box plot, compare the stopping times of all 4 algorithms (UCB, uniform, TTUCB and EB-TC) and comment the results.
\end{parts}

\question (Bonus question) Thompson Sampling is another regret minimization algorithm. Implement a Top Two algorithm which uses Thompson Sampling. Give a pseudo-code of your algorithm. Compare that algorithm to the others on various bandit problems.

\end{questions}
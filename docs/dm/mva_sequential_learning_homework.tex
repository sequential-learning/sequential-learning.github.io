% \documentclass[answers,11pt]{exam}
\documentclass[11pt]{exam}


\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\usepackage{dsfont}
\usepackage{url}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{graphicx}
% \usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{color}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage[ruled]{algorithm2e}
\usepackage{listings}
\lstset{language=python,basicstyle={\ttfamily}}

\hypersetup{
colorlinks,
citecolor=blue,
filecolor=blue,
linkcolor=blue,
urlcolor=black
}
% \pagestyle{fancy}
\geometry{verbose,tmargin=1.0in,bmargin=1.0in,lmargin=0.5in,rmargin=0.5in}
\setlength{\headheight}{15pt}
% \firstpageheadrule
\runningheadrule
\shadedsolutions

\newcommand{\thiscoursecode}{Sequential Learning}
\newcommand{\examname}{Home Assignment}
\newcommand{\thisprof}{Pierre Gaillard}
\newcommand{\remy}{R\'emy Degenne}
\newcommand{\me}{Pierre Gaillard}
\newcommand{\thisterm}{MVA 2022-23}
\newcommand{\website}{\url{https://sequential-learning.github.io/}}
\newcommand{\email}{pierre.gaillard@inria.fr}
\newcommand{\emailremy}{remy.degenne@inria.fr}

\usepackage{newfloat} % changer les noms des legendes
\DeclareFloatingEnvironment{algo}
\renewcommand{\algoname}{Algorithm}%

\DeclareFloatingEnvironment{setting}
\renewcommand{\settingname}{Setting}%

% Headers
\chead{\thiscoursename }
\lhead{\thisterm}
\rhead{\thisprof}

\firstpageheader{MVA 2022-23}{}{\thisprof, \  \remy}
\runningheader{MVA 2022-23}{\examname}{\thisprof, \  \remy}

\setenumerate{itemsep=0pt,topsep=-2pt}
\setitemize{label={--},nosep,nolistsep,topsep=-2pt}

%%%%%% TITLE %%%%%%
\newcommand{\notefront}{
%\pagenumbering{roman}
\begin{center}

\hrule \vspace*{20pt}
\textbf{\huge{\textsc{\thiscoursecode}}}{\huge \par} 

\smallskip
{\large{\textsc{\examname}}}\\ \vspace{20pt}

\hrule \vspace*{20pt}

  \end{center}
  }


\setlength\parindent{0pt}

% Commandes de pierre
\newcommand{\indic}{\mathds{1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\S}{\mathbb{S}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cN}{\mathcal{N}}
\DeclareMathOperator*{\Proj}{Proj}

% Begin Document
\begin{document}
  % Notes front
  \notefront
  % Table of Contents and List of Figures
  % Abstract


This homework should be uploaded by {\bfseries Friday, March 17, 2023} as a pdf file on the website
\begin{center}
	\url{https://sequential-learning.github.io/}
\end{center}
The password to upload is \texttt{mva2023}. The penalty scale is minus two points (on the final grade over 20 points) for every day of delay. The homework can be done alone or in groups of two students. The code can be done in any langage (\texttt{python}, \texttt{R}, \texttt{matlab},\dots) and should not be returned but the results and the figures must be included into the pdf report. 

\medskip
All questions require a proper mathematical justification or derivation (unless otherwise stated), but most questions can be answered concisely in just a few lines. No question should require lengthy or tedious derivations or calculations.



\section*{Part 1. Bandit convex optimization}

We consider the following setting of online zero-order convex optimization. Let $\Theta \subseteq \R^d$ be a convex decision set that contains the unit ball and with diameter $D := \sup_{\theta,\theta' \in \Theta}\|\theta-\theta'\|$. 
First, the environment chooses a sequence of convex and $G$-Lipschitz loss functions $\ell_t:\Theta \to [-1,1]$, initially hidden from the learner. At each round $t\geq 1$, the learner is asked to choose an action $\theta_t \in \Theta$ and observes its loss $\ell_t(\theta_t)$. We consider the following algorithm. 
\begin{algorithm}[!h]
	\SetAlgoLined
	Input: decision set $\Theta$, $\delta > 0$ and $\eta > 0$ \\
	Initialization: $\hat \theta_1  = 0$ \\
	\For{t=1,\dots,T}{
		Draw $u_t \in \S_1$ uniformly at random and set $\theta_t = \hat \theta_t + \delta u_t$ \\
		Play $\theta_t$ and observes $\ell_t(\theta_t)$ \\
		Update $\hat \theta_{t+1} = \Proj_{\Theta_\delta}\big[\hat \theta_t - \frac{d \eta}{\delta} \ell_t(\theta_t) u_t \big]$ where $\Proj_{\Theta_\delta}$ is the Euclidean projection on~$\Theta_\delta := \{\theta, \frac{\theta}{1-\delta} \in \Theta\}$.
	}
	\caption{OGD without gradients}
	\label{alg:ogdnogradients}
\end{algorithm}


\begin{questions}
	\question Is the adversary oblivious or adaptive in this setting?
	\question Write the definition of the regret $R_T$.
	\question Define the $\delta$-smoothed version of $\ell_t(\theta)$ by $\hat \ell_t(\theta) := \E_{v}[\ell_t(\theta + \delta v)]$ where $v$ is uniformly sampled on the unit ball. 
	\begin{parts}
		\part Show that when $d=1$, 
	\[
		\E_{u_t}\Big[ \frac{d}{\delta} \ell_t(\hat \theta_t + \delta u_t) u_t \Big] = \nabla \hat \ell_t(\hat \theta_t) \,.
	\]
	We admit that the results holds for all $d\geq 1$ (this can be proved by using Stoke's theorem).
		\part Show that for any $\theta \in \Theta_\delta$,
	\[
			\big| \hat \ell_t(\theta) - \ell_t(\theta) \big| \leq G \delta.
	\]
	\end{parts}
	\question Define $h_t(\theta) = \hat \ell_t(\theta) + \langle \xi_t, \theta \rangle$, where $\xi_t = \frac{d}{\delta} \ell_t(\theta_t) u_t - \nabla \hat \ell_t(\hat \theta_t)$. 
	\begin{parts}
		\part What would be the updates of Online Gradient Descent applied to the sequence of losses $h_t$?
		\part Show that for any $\theta^*_\delta \in \Theta_\delta$
				\[
					\sum_{t=1}^T h_t(\hat \theta_t) - h_t(\theta^*_\delta) \leq \eta \frac{d^2}{\delta^2} T + \frac{D^2}{\eta} \,.
				\]
		\part Deduce that for any $\theta^*_\delta \in \Theta_\delta$
				\[
					\sum_{t=1} \E\big[ \hat \ell_t(\hat \theta_t)\big] - \hat \ell_t(\theta^*_\delta)  \leq \eta \frac{d^2}{\delta^2} T + \frac{D^2}{\eta}  \,.
				\]
	\end{parts} 
	\question Conclude that 
			\[
				\E\big[R_T\big] \leq \eta \frac{d^2}{\delta^2} T + \frac{D^2}{\eta} + 4 \delta DGT \,.
			\]
	\question What regret do we obtain by optimizing the parameters $\delta$ and $\eta$? 

	\question In this question, we perform some simulations to assess the empirical performance of the algorithm and compare it with standard Online Gradient Descent. Let $d,T \geq 1$. We define $\theta^*_i = \frac{1}{2i}$, and  let for $t=1,\dots, T$, $x_t \sim \cN(0,I_d)$ and $y_t = \langle \theta^*, x_t \rangle + \epsilon_t$, with $\epsilon_t \sim \cN(0,.1)$. At each round, the learner is asked to form $\theta_t \in \R^d$ and is evaluated with the loss $\ell_t(\theta) = (\langle\theta,x_t\rangle - y_t)^2$. 

	\begin{parts}
		\part Let $d = 2$. Implement OGD with and without gradients for $\Theta = \{\|\theta\|\leq 1\}$. Plot the cumulative regrets obtained for theoretical values of $\eta$ and $\delta$ for $n=1,\dots,1000$. Add standard deviation obtained over 100 runs of the experiment to the plots.
		\part Fix $n=1000$. Plot the regret with standard deviation as a function of $d=1,\dots,10$. What do you observe?
	\end{parts}

\end{questions}

\input{bandit_identification}

 % \bibliographystyle{plain}
 % \bibliography{biblio}




\end{document}
